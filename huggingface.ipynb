{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we install the transformers library and pytorch\n",
    "#this is the general workflow for modern deep learning type projects\n",
    "\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "#distilbert is a smaller version of bert, but trained on the same data and optimized\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can now tokenize a sequence of text\n",
    "\n",
    "tokenized_sequence = tokenizer.tokenize(\"test blargeldy their, they're\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'b', '##lar', '##gel', '##dy', 'their', ',', 'they', \"'\", 're']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#notice how bert handles contractions and unseen words\n",
    "tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\", return_tensors='pt')\n",
    "\n",
    "output=model(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3919e-01, -7.1982e-02,  4.0344e-02, -2.2243e-01, -1.7465e-01,\n",
      "        -1.6524e-01,  2.8922e-01,  3.8080e-01, -2.0477e-01, -1.9203e-01,\n",
      "        -8.0516e-02, -6.3089e-02, -2.2332e-01,  1.5912e-01,  1.7414e-01,\n",
      "         1.2588e-01, -8.3952e-02,  1.9253e-01,  1.1660e-01, -1.1700e-01,\n",
      "        -1.6660e-01, -8.2792e-02,  3.4711e-02, -4.5443e-02,  3.5760e-02,\n",
      "        -6.7369e-02, -1.0968e-01, -6.6397e-02, -4.2676e-02,  1.0038e-02,\n",
      "        -7.6260e-03,  1.0621e-01, -8.4307e-02, -5.1339e-02,  3.1836e-02,\n",
      "         1.4903e-02,  7.4059e-02, -8.4516e-03,  1.0604e-01, -1.2139e-02,\n",
      "        -1.7827e-01,  8.6443e-02,  1.7273e-01, -2.7619e-02, -8.3613e-02,\n",
      "        -2.3327e-01, -2.4096e+00, -7.7631e-02, -2.2396e-01, -2.3699e-01,\n",
      "         9.3535e-02,  5.2516e-02, -4.8690e-02,  2.9909e-01,  1.2537e-01,\n",
      "         2.2622e-01, -4.0399e-01,  3.3692e-01, -4.6337e-02, -3.3084e-02,\n",
      "         1.3035e-01, -3.1199e-02, -9.2113e-03,  9.7409e-02, -9.4188e-02,\n",
      "         1.8251e-01,  2.2600e-02,  2.8169e-01, -3.9132e-01,  4.0046e-01,\n",
      "        -1.7328e-01, -2.5700e-01,  1.5185e-01, -2.8525e-02, -2.8561e-02,\n",
      "        -1.8280e-01, -7.8613e-02,  1.5033e-01, -1.1830e-01,  1.3466e-01,\n",
      "        -1.5145e-01,  9.6336e-02,  2.1002e-01,  2.5609e-02,  3.4635e-02,\n",
      "         9.2930e-02, -2.4561e-01, -8.4071e-02,  1.7593e-01,  3.9299e-01,\n",
      "        -1.4460e-01, -2.7592e-02, -5.5245e-03,  2.3321e-01,  4.3507e-01,\n",
      "        -3.2466e-01,  6.5001e-02, -2.6603e-02,  2.9555e-01,  1.6735e-01,\n",
      "         7.8335e-02, -2.5279e-01,  2.2146e-01, -5.0358e-01, -1.7381e-02,\n",
      "        -6.9579e-02, -5.3796e-02, -2.7489e-01,  1.8497e-01, -2.4786e+00,\n",
      "         2.8336e-01,  1.7334e-01, -2.8461e-02, -4.1359e-01, -1.1338e-01,\n",
      "         3.6591e-01,  2.9488e-01,  1.1828e-01,  1.4192e-01, -2.3308e-03,\n",
      "        -1.4947e-01,  3.7960e-01,  1.1833e-01, -1.3505e-01,  1.7801e-02,\n",
      "         2.2082e-01,  4.2742e-02,  7.3596e-02,  5.5133e-02,  5.8120e-02,\n",
      "         2.4742e-01,  3.5982e-01,  1.5366e-01, -4.6615e-02, -1.5883e-01,\n",
      "         1.6264e-01,  2.2342e-01, -1.2428e-01, -1.9376e-01, -6.6524e-02,\n",
      "        -2.4706e-01, -1.3274e-02, -2.8957e+00,  3.3832e-01,  4.7032e-01,\n",
      "         1.1278e-02, -1.7949e-01,  5.9760e-03,  1.2925e-01,  2.4715e-01,\n",
      "         1.9417e-01,  1.6383e-01, -7.0799e-02,  6.6086e-02, -2.5521e-01,\n",
      "         2.6850e-02, -1.1169e-01, -6.0153e-02,  1.1534e-01,  2.4401e-01,\n",
      "         1.4863e-01, -6.6402e-03, -6.3334e-02, -3.7520e-02, -2.3890e-02,\n",
      "         1.3183e-01,  2.9880e-01,  1.0376e-01,  1.4208e-01, -2.8020e-02,\n",
      "        -1.6707e-01, -1.7437e-01,  2.6417e-01, -1.7912e-02,  1.9477e-01,\n",
      "        -9.3709e-03,  1.7929e-01,  2.0153e-01,  1.4229e-01, -1.7261e-01,\n",
      "        -1.3144e-02,  2.6531e-01,  2.8453e-02,  8.6874e-02,  4.6339e-02,\n",
      "         7.6937e-02,  2.6317e-01, -2.4253e-01, -7.0050e-02,  2.6369e-01,\n",
      "         2.3880e-02, -8.8716e-02,  1.4582e-01, -8.7686e-02,  3.1599e-01,\n",
      "         7.0351e-02, -2.3755e-02, -3.2746e-01,  1.5900e-01,  1.9576e-01,\n",
      "        -1.2654e-01,  7.4058e-02, -8.4669e-02, -2.0820e-04, -2.7644e-02,\n",
      "         3.7200e+00, -1.8582e-02,  1.0314e-01,  7.3107e-02,  1.6572e-01,\n",
      "        -2.0280e-01,  1.7263e-01, -5.0444e-02, -1.0678e-01,  1.3982e-01,\n",
      "         6.0039e-02,  3.2774e-01, -6.4981e-02,  1.6139e-01, -1.7841e-01,\n",
      "         1.1108e-01,  3.1795e-02, -5.2136e-02,  1.0801e-01, -1.2523e-01,\n",
      "         6.3277e-02,  1.8315e-01,  2.1978e-01,  1.4658e-01, -1.3313e+00,\n",
      "         9.0358e-02,  1.1077e-02, -9.7506e-05,  3.5392e-01, -1.9085e-01,\n",
      "        -1.0550e-01,  7.2041e-02,  2.1009e-03,  4.4596e-02,  6.6291e-02,\n",
      "        -3.4387e-02,  2.7084e-01,  2.1404e-01,  2.5628e-01, -2.6163e-01,\n",
      "         3.0559e-01,  8.9785e-02, -4.9575e-02,  1.0531e-01, -4.6621e-02,\n",
      "         1.5697e-01, -1.5009e-02, -7.7601e-04, -1.9511e-01,  4.2408e-02,\n",
      "        -4.7011e-02,  3.5413e-02,  2.3699e-01, -2.8632e-01, -1.4093e-01,\n",
      "        -3.2077e-01, -8.7625e-02,  1.8315e-02,  2.1054e-02, -3.5050e-01,\n",
      "        -1.4362e-01,  8.1928e-02, -1.6225e-01,  7.9584e-02,  4.7455e-02,\n",
      "        -2.2766e-01, -1.5728e-01, -2.1964e-01, -3.9822e+00, -6.5706e-03,\n",
      "        -2.5282e-02,  2.5725e-01,  3.2836e-01, -1.1669e-01,  2.5196e-02,\n",
      "         4.5992e-04,  1.2061e-01, -3.8558e-01,  4.5332e-01,  2.6559e-01,\n",
      "        -3.0758e-02,  2.8822e-01, -3.1095e-01,  1.1369e-01,  4.2511e-02,\n",
      "        -1.2857e-01, -2.1752e-01, -2.1967e-01, -1.0442e-01,  3.2286e-01,\n",
      "        -1.2452e-01,  1.6432e-01,  1.3565e-01, -1.7473e-01, -8.3161e-02,\n",
      "        -1.4045e-01,  2.5256e-02, -5.5887e-02, -8.7918e-02, -2.7393e-01,\n",
      "         7.1362e-02, -1.6937e-01, -1.3759e-01, -2.4734e+00, -4.7415e-02,\n",
      "        -7.6085e-02, -5.1072e-02,  9.0534e-02, -5.1241e-02,  3.1597e-01,\n",
      "        -1.0087e-01, -3.0251e-02, -4.7838e-02,  1.6348e-01, -1.3350e-01,\n",
      "        -1.0833e-01,  9.9630e-02,  1.9459e-01,  3.9824e-02,  7.5784e-02,\n",
      "        -1.5257e-02,  1.2402e-01,  1.9453e-01,  2.1549e-02, -8.4674e-02,\n",
      "        -4.3155e-02, -7.0183e-02,  5.2103e-02,  3.7489e-01, -2.9911e-01,\n",
      "        -8.1262e-02, -7.2050e-02, -8.4138e-02, -1.5952e-02, -1.8200e-01,\n",
      "         6.9827e-02, -9.0961e-02, -3.4001e-01, -2.5302e-01,  1.8666e-01,\n",
      "         1.3637e-01,  4.4387e-01,  2.8827e-02, -4.5555e-02,  5.6179e-01,\n",
      "         1.6965e-01,  6.9841e-02,  2.7657e-01,  8.9140e-02, -8.4785e-02,\n",
      "        -2.1051e-01, -8.4015e-02,  9.6318e-03, -1.7119e-01,  1.1620e-01,\n",
      "         1.1058e+00, -1.0784e-01,  2.1273e-01, -1.4945e-01,  2.7866e-01,\n",
      "         7.1243e-02,  3.1684e-01,  8.7782e-02,  4.3964e-01, -2.0687e-01,\n",
      "         2.0952e-01, -1.6289e-01,  7.1974e-02, -2.6707e-01,  4.3451e-02,\n",
      "        -2.4404e-01,  1.0633e-01,  6.7527e-02, -1.2398e-01,  1.6130e-01,\n",
      "        -2.6070e-02, -7.9852e-01, -2.7527e-01,  2.2771e-02, -1.0165e-01,\n",
      "         2.1490e-02,  4.1520e-02,  1.7851e-02, -2.5656e-01, -1.0597e-01,\n",
      "        -1.9662e-01,  3.0549e-01, -1.3683e-01, -6.4406e-02, -7.4870e-02,\n",
      "         4.7274e-03, -3.0009e-01,  1.2581e-01,  3.2337e-03,  1.9633e-01,\n",
      "         8.2094e-02,  1.3347e-01,  2.0888e-02,  4.1445e-03,  2.3527e-01,\n",
      "        -8.5230e-01,  1.5046e-01, -1.3584e-01,  1.1995e-01, -2.4940e-01,\n",
      "        -6.5267e-02,  9.4236e-02, -1.3357e-01, -2.2408e-02, -3.8466e-01,\n",
      "         2.9671e-01, -5.0464e-02,  2.1763e-01, -4.2999e-02,  1.3679e-01,\n",
      "        -2.4204e-01,  9.9700e-02,  8.4853e-01, -1.7666e-03, -1.6305e-01,\n",
      "         2.0386e-01, -1.0814e-02,  2.2161e-01,  1.1000e-01,  1.3633e-01,\n",
      "        -1.7514e-01, -4.1611e-02, -1.2767e-01, -1.6987e-02,  3.7809e-02,\n",
      "        -7.6339e-02, -8.4406e-02, -1.3929e-01,  2.8656e-02,  4.9726e-02,\n",
      "        -1.7593e-01, -5.4656e-01,  4.8031e-02, -1.4726e-01, -5.4376e-02,\n",
      "         1.8679e-01,  3.5925e-01,  1.0144e-01,  2.6011e-01,  1.3205e-01,\n",
      "        -1.3046e-01,  3.5581e-01, -1.0329e-01,  3.6639e-01, -6.4490e-02,\n",
      "         6.8404e-02, -9.9475e-02,  2.7666e-01,  7.8361e-02, -2.6574e-01,\n",
      "         9.1610e-02, -3.1323e-01,  1.7438e-01,  1.2368e-01,  4.2416e-02,\n",
      "        -3.6154e-02, -6.7831e-02,  1.7847e-01,  1.5478e-01,  6.8344e-02,\n",
      "        -1.3028e+00,  1.9813e-01,  1.8130e-01,  2.0410e-02, -5.2896e-02,\n",
      "        -4.1703e-02, -2.4024e-01,  3.5582e-01, -3.2711e-02,  7.7840e-02,\n",
      "        -4.8011e-02,  2.1984e-01,  4.2985e-02, -1.3905e-01,  4.6935e-02,\n",
      "        -8.7698e-03,  1.8612e-01,  5.3053e-02,  4.6904e-02,  6.9313e-02,\n",
      "         5.8655e-03,  2.5985e-01,  7.6216e-02, -2.1480e-01,  1.0914e-01,\n",
      "        -2.7757e-02,  5.6701e-02,  1.3398e-01,  6.8672e-02,  1.2076e-01,\n",
      "        -5.0242e-02, -3.6281e-01, -4.9417e-01, -3.0425e-01,  5.7694e-02,\n",
      "        -6.4055e-02,  2.0128e-02,  3.9044e-02,  2.7619e-01,  2.3790e-01,\n",
      "        -3.2463e-01,  9.4494e-02,  7.1146e-02, -1.2498e-01,  4.8001e-01,\n",
      "         4.0109e-02, -1.0753e-01,  2.6765e-01,  5.1166e-02, -2.7893e-01,\n",
      "        -8.7614e-02, -2.4310e-01, -7.6241e-02, -1.8392e-01, -1.4080e-02,\n",
      "        -1.1762e-01, -6.3120e-02,  5.6924e-02, -1.8846e-01, -1.6521e-01,\n",
      "         2.6139e-01, -2.1921e-01, -1.1264e-01,  2.5025e-01, -3.1218e-01,\n",
      "        -5.2908e-01,  4.9078e-02, -2.0269e-01, -4.4714e-02,  1.7296e-02,\n",
      "         4.3910e-01,  5.6017e-02, -1.8709e-01, -7.2760e-02, -3.0679e-01,\n",
      "         1.1640e-01, -1.3459e-01,  1.4483e-01,  1.8197e-01, -1.8583e-01,\n",
      "         6.8724e-03, -3.9139e-01, -1.2347e-01,  1.5015e-01,  4.5091e-02,\n",
      "         1.0311e-01,  1.2680e-03,  7.8706e-02,  4.0956e-02,  1.2831e-01,\n",
      "        -2.2103e-01, -1.4388e-01,  9.1448e-03,  6.1248e-02,  4.8158e-02,\n",
      "        -8.9238e-03,  7.3165e-02, -6.2242e-02, -5.2814e-02, -1.0124e-01,\n",
      "        -9.1000e-02,  3.9056e-01,  1.5298e-01,  2.5435e-01,  8.9818e-02,\n",
      "         1.9950e-01,  2.6668e-01,  2.4115e-01, -1.7151e-01, -1.6855e-01,\n",
      "         3.4427e-03,  2.6042e-02, -2.8881e-02, -4.6264e-03, -4.0418e-02,\n",
      "         4.7408e-02,  4.1626e-02, -1.0929e-01,  2.1309e+00,  3.5698e-01,\n",
      "        -9.6435e-02,  3.5292e-02,  2.4609e-01,  3.4645e-02, -3.4345e-02,\n",
      "         8.2340e-02,  1.1200e-02,  3.5091e-01, -3.1817e-01,  2.8757e-01,\n",
      "        -6.9868e-02,  1.3769e-01,  3.1216e-01,  1.9134e-01,  2.8628e-01,\n",
      "        -3.6709e-02, -3.5320e-01, -6.6039e-02, -2.7269e-01,  2.1127e-01,\n",
      "         2.7397e-01,  3.0363e-03, -1.6556e-01,  1.5154e-01,  2.3344e-01,\n",
      "        -1.0508e-01, -4.6845e-02,  2.3854e-01,  2.3465e-03,  6.0589e-02,\n",
      "         1.9571e-01,  1.5446e-01, -6.3045e-02,  3.8478e-02,  1.7208e-01,\n",
      "        -3.1523e-01, -2.5292e-01, -6.2427e-04,  2.8621e-02, -3.7261e-01,\n",
      "         3.3550e-01, -4.3340e-03, -1.7640e-01,  4.3110e-01,  1.6436e-02,\n",
      "        -2.5587e-01,  1.8779e-01,  1.5795e-01, -9.8425e-03,  6.0727e-02,\n",
      "        -2.4666e-01,  1.6165e-01, -1.8792e-01, -2.6889e-01,  4.1250e-02,\n",
      "         6.0947e-02,  7.9321e-02,  3.2852e-01,  2.9557e-02,  2.1318e-01,\n",
      "         2.5646e-01, -1.1954e-01,  4.0068e-02, -1.0843e-01, -1.6183e-01,\n",
      "         1.6766e-01,  4.7462e-02, -4.3834e-02, -2.6199e-02,  4.0442e-01,\n",
      "         2.2421e-01,  9.8809e-02,  3.7208e-01,  1.8254e-03,  1.4145e-01,\n",
      "        -3.5901e-02, -5.7009e-02, -2.7743e+00,  7.6840e-02,  1.7237e-01,\n",
      "         2.9272e-01,  1.7541e-02,  3.1325e-01,  2.2523e-01,  6.3337e-03,\n",
      "         2.1454e-01,  2.2922e-02,  1.4191e-01,  2.3854e-01,  3.6752e-01,\n",
      "         3.1767e-02,  1.9138e-01, -1.3762e-01,  1.6235e-01, -1.7898e-01,\n",
      "         5.0364e-02, -1.3660e-01, -1.7045e-03,  1.8108e-02,  3.9234e-02,\n",
      "        -1.9992e-01, -2.4388e-01,  4.0501e-02, -1.5268e-01, -9.3294e-02,\n",
      "         5.5471e-02,  6.4216e-02, -8.7321e-02,  3.3429e-01, -3.1434e-01,\n",
      "        -6.2192e-02,  9.5608e-02, -7.4223e-02, -7.6549e-02, -3.0532e-02,\n",
      "         1.5672e-01,  1.6836e-01, -5.0349e-02,  4.8621e-01, -1.7264e-01,\n",
      "         3.4014e-01, -9.2879e-02,  1.1149e-01,  3.4611e-01, -8.8136e-02,\n",
      "         1.7258e-01, -1.8959e-01, -1.4069e-01,  1.7311e-01,  1.0114e-01,\n",
      "        -2.1450e-02,  1.3094e-01,  5.9128e-02,  1.1008e-01, -9.7970e-02,\n",
      "        -8.6286e-02, -2.3214e-01,  1.9072e-02, -3.0637e-02,  1.3116e-01,\n",
      "        -1.9873e-01,  2.7195e-01, -9.9676e-02, -1.3715e-01,  8.7482e-02,\n",
      "        -1.2156e-01, -1.5251e-01, -2.2310e-01, -3.1479e-02,  1.5980e-01,\n",
      "         2.0038e-01,  8.9643e-02,  4.6280e-02,  2.4838e-01,  1.5667e-01,\n",
      "         3.0736e-02,  1.1984e-01, -7.4758e-02, -2.5533e-01,  1.5573e-01,\n",
      "        -1.6527e-01,  1.2995e-01, -7.9235e+00, -1.8371e-01, -1.3878e-01,\n",
      "        -2.5917e-01,  2.6410e-02, -1.7710e-01, -7.8502e-02, -1.7300e-01,\n",
      "         1.3243e-01, -7.1294e-02,  6.0726e-02,  8.5444e-02, -7.4497e-02,\n",
      "        -5.6379e-02,  2.8349e-01,  3.0240e-01], grad_fn=<SelectBackward0>)\n",
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "#value for the classification token, we can use this to do downstream nlp tasks\n",
    "print(output.last_hidden_state[0][0])\n",
    "print(output.last_hidden_state[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=tokenizer(\"She withdrew money from the bank.\", \"She fished by the river bank.\", return_tensors='pt')\n",
    "\n",
    "output=model(**input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8869, grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "#calculating the cosine similarity between the two sentences\n",
    "import torch.nn.functional as F\n",
    "\n",
    "cosine_similarity = F.cosine_similarity(output.last_hidden_state[0][6], output.last_hidden_state[0][14], dim=0)\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9662, grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "input=tokenizer(\"She withdrew money from the bank.\", \"She made a deposit at the bank.\", return_tensors='pt')\n",
    "output=model(**input)\n",
    "cosine_similarity = F.cosine_similarity(output.last_hidden_state[0][6], output.last_hidden_state[0][15], dim=0)\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05292845517396927,\n",
       "  'token': 2535,\n",
       "  'token_str': 'role',\n",
       "  'sequence': \"hello i ' m a role model.\"},\n",
       " {'score': 0.03968577831983566,\n",
       "  'token': 4827,\n",
       "  'token_str': 'fashion',\n",
       "  'sequence': \"hello i ' m a fashion model.\"},\n",
       " {'score': 0.034743569791316986,\n",
       "  'token': 2449,\n",
       "  'token_str': 'business',\n",
       "  'sequence': \"hello i ' m a business model.\"},\n",
       " {'score': 0.03462294116616249,\n",
       "  'token': 2944,\n",
       "  'token_str': 'model',\n",
       "  'sequence': \"hello i ' m a model model.\"},\n",
       " {'score': 0.01814522221684456,\n",
       "  'token': 11643,\n",
       "  'token_str': 'modeling',\n",
       "  'sequence': \"hello i ' m a modeling model.\"}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are tons of premade and pretrained pipelines in huggingface\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model=\"distilbert-base-uncased\")\n",
    "\n",
    "unmasker(\"Hello I'm a [MASK] model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.11590294539928436,\n",
       "  'token': 10533,\n",
       "  'token_str': 'carpenter',\n",
       "  'sequence': 'the man worked as a carpenter.'},\n",
       " {'score': 0.0723821222782135,\n",
       "  'token': 20987,\n",
       "  'token_str': 'blacksmith',\n",
       "  'sequence': 'the man worked as a blacksmith.'},\n",
       " {'score': 0.05382770299911499,\n",
       "  'token': 22701,\n",
       "  'token_str': 'tailor',\n",
       "  'sequence': 'the man worked as a tailor.'},\n",
       " {'score': 0.0413505956530571,\n",
       "  'token': 15610,\n",
       "  'token_str': 'waiter',\n",
       "  'sequence': 'the man worked as a waiter.'},\n",
       " {'score': 0.03554900735616684,\n",
       "  'token': 14998,\n",
       "  'token_str': 'butcher',\n",
       "  'sequence': 'the man worked as a butcher.'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also examine some biases in the model\n",
    "unmasker(\"The man worked as a [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.12517952919006348,\n",
       "  'token': 6821,\n",
       "  'token_str': 'nurse',\n",
       "  'sequence': 'the woman worked as a nurse.'},\n",
       " {'score': 0.08857159316539764,\n",
       "  'token': 10850,\n",
       "  'token_str': 'maid',\n",
       "  'sequence': 'the woman worked as a maid.'},\n",
       " {'score': 0.07708469033241272,\n",
       "  'token': 13877,\n",
       "  'token_str': 'waitress',\n",
       "  'sequence': 'the woman worked as a waitress.'},\n",
       " {'score': 0.054324790835380554,\n",
       "  'token': 10533,\n",
       "  'token_str': 'carpenter',\n",
       "  'sequence': 'the woman worked as a carpenter.'},\n",
       " {'score': 0.04624558985233307,\n",
       "  'token': 22583,\n",
       "  'token_str': 'housekeeper',\n",
       "  'sequence': 'the woman worked as a housekeeper.'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"The woman worked as a [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.027150411158800125,\n",
       "  'token': 6014,\n",
       "  'token_str': 'heaven',\n",
       "  'sequence': 'a person from heaven is an enemy.'},\n",
       " {'score': 0.016995996236801147,\n",
       "  'token': 28978,\n",
       "  'token_str': 'afar',\n",
       "  'sequence': 'a person from afar is an enemy.'},\n",
       " {'score': 0.012630046345293522,\n",
       "  'token': 3607,\n",
       "  'token_str': 'russia',\n",
       "  'sequence': 'a person from russia is an enemy.'},\n",
       " {'score': 0.011639977805316448,\n",
       "  'token': 3577,\n",
       "  'token_str': 'spain',\n",
       "  'sequence': 'a person from spain is an enemy.'},\n",
       " {'score': 0.010551409795880318,\n",
       "  'token': 7041,\n",
       "  'token_str': 'afghanistan',\n",
       "  'sequence': 'a person from afghanistan is an enemy.'}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"A person from [MASK] is an enemy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.019114704802632332,\n",
       "  'token': 6014,\n",
       "  'token_str': 'heaven',\n",
       "  'sequence': 'a person from heaven is lazy.'},\n",
       " {'score': 0.01292795967310667,\n",
       "  'token': 3088,\n",
       "  'token_str': 'africa',\n",
       "  'sequence': 'a person from africa is lazy.'},\n",
       " {'score': 0.011858571320772171,\n",
       "  'token': 28978,\n",
       "  'token_str': 'afar',\n",
       "  'sequence': 'a person from afar is lazy.'},\n",
       " {'score': 0.009887747466564178,\n",
       "  'token': 7880,\n",
       "  'token_str': 'nowhere',\n",
       "  'sequence': 'a person from nowhere is lazy.'},\n",
       " {'score': 0.009876523166894913,\n",
       "  'token': 2634,\n",
       "  'token_str': 'india',\n",
       "  'sequence': 'a person from india is lazy.'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"A person from [MASK] is lazy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POS', 'score': 0.9926134347915649}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#notice that our model doesn't need any preprocessing anymore\n",
    "sentiment_model(\"I love this movie!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems=pd.read_csv(\"kaggle_poem_dataset.csv\").head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POS', 'score': 0.9428346157073975}]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sentiment_model(poems[\"Content\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#short context window, so only keep first part of each poem\n",
    "\n",
    "poems[\"Content\"]=poems[\"Content\"].apply(lambda x:x[:300])\n",
    "\n",
    "sentiments=[]\n",
    "for poem in poems[\"Content\"]:\n",
    "  sentiments.append(sentiment_model(poem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract labels from sentiments\n",
    "\n",
    "sentiments=[sentiment[0][\"label\"] for sentiment in sentiments]\n",
    "poems[\"Sentiment\"]=sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poetry Foundation ID</th>\n",
       "      <th>Content</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Wendy Videlock</td>\n",
       "      <td>!</td>\n",
       "      <td>55489</td>\n",
       "      <td>Dear Writers, I’m compiling the first in what ...</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Hailey Leithauser</td>\n",
       "      <td>0</td>\n",
       "      <td>41729</td>\n",
       "      <td>Philosophic\\nin its complex, ovoid emptiness,\\...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Jody Gladding</td>\n",
       "      <td>1-800-FEAR</td>\n",
       "      <td>57135</td>\n",
       "      <td>We'd  like  to  talk  with  you  about  fear t...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Joseph Brodsky</td>\n",
       "      <td>1 January 1965</td>\n",
       "      <td>56736</td>\n",
       "      <td>The Wise Men will unlearn your name.\\nAbove yo...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ted Berrigan</td>\n",
       "      <td>3 Pages</td>\n",
       "      <td>51624</td>\n",
       "      <td>For Jack Collom\\n10 Things I do Every Day\\n\\np...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>Myung Mi Kim</td>\n",
       "      <td>[accumulation of land]\\n \\n \\n  \\n   Launch Au...</td>\n",
       "      <td>53678</td>\n",
       "      <td>accumulation of land              maintain hou...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>Cally Conan-Davies</td>\n",
       "      <td>Ace</td>\n",
       "      <td>55551</td>\n",
       "      <td>Bloody hell, the world’s turned\\nupside down\\n...</td>\n",
       "      <td>NEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>Robert Gernhardt</td>\n",
       "      <td>Ach/Last Call</td>\n",
       "      <td>40352</td>\n",
       "      <td>Right up to my final hour\\nI'll be obliging an...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>Denise Levertov</td>\n",
       "      <td>The Ache of Marriage</td>\n",
       "      <td>42523</td>\n",
       "      <td>The ache of marriage:\\n\\nthigh and tongue, bel...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>Danez Smith</td>\n",
       "      <td>acknowledgments</td>\n",
       "      <td>148357</td>\n",
       "      <td>you save me half a bag of skins, the hard part...</td>\n",
       "      <td>NEU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0              Author  \\\n",
       "0            0      Wendy Videlock   \n",
       "1            1   Hailey Leithauser   \n",
       "2            2       Jody Gladding   \n",
       "3            3      Joseph Brodsky   \n",
       "4            4        Ted Berrigan   \n",
       "..         ...                 ...   \n",
       "95          95        Myung Mi Kim   \n",
       "96          96  Cally Conan-Davies   \n",
       "97          97    Robert Gernhardt   \n",
       "98          98     Denise Levertov   \n",
       "99          99         Danez Smith   \n",
       "\n",
       "                                                Title  Poetry Foundation ID  \\\n",
       "0                                                   !                 55489   \n",
       "1                                                   0                 41729   \n",
       "2                                          1-800-FEAR                 57135   \n",
       "3                                      1 January 1965                 56736   \n",
       "4                                             3 Pages                 51624   \n",
       "..                                                ...                   ...   \n",
       "95  [accumulation of land]\\n \\n \\n  \\n   Launch Au...                 53678   \n",
       "96                                                Ace                 55551   \n",
       "97                                      Ach/Last Call                 40352   \n",
       "98                               The Ache of Marriage                 42523   \n",
       "99                                    acknowledgments                148357   \n",
       "\n",
       "                                              Content Sentiment  \n",
       "0   Dear Writers, I’m compiling the first in what ...       POS  \n",
       "1   Philosophic\\nin its complex, ovoid emptiness,\\...       NEU  \n",
       "2   We'd  like  to  talk  with  you  about  fear t...       NEU  \n",
       "3   The Wise Men will unlearn your name.\\nAbove yo...       NEU  \n",
       "4   For Jack Collom\\n10 Things I do Every Day\\n\\np...       NEU  \n",
       "..                                                ...       ...  \n",
       "95  accumulation of land              maintain hou...       NEU  \n",
       "96  Bloody hell, the world’s turned\\nupside down\\n...       NEG  \n",
       "97  Right up to my final hour\\nI'll be obliging an...       NEU  \n",
       "98  The ache of marriage:\\n\\nthigh and tongue, bel...       NEU  \n",
       "99  you save me half a bag of skins, the hard part...       NEU  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from sentence-transformers) (4.45.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from sentence-transformers) (0.25.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucia\\Documents\\GitHub\\huggingface-demo\\.conda\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lucia\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Sentences we want to encode. Example:\n",
    "sentence = ['This framework generates embeddings for each input sentence']\n",
    "\n",
    "# Sentences are encoded by calling model.encode()\n",
    "embedding = model.encode(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6667714]], dtype=float32)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate cosine similarity between two sentences\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sentence1 = \"I love apples\"\n",
    "sentence2 = \"I hate apples\"\n",
    "# Encode sentences to get their embeddings\n",
    "\n",
    "embedding1 = model.encode(sentence1)\n",
    "\n",
    "embedding2 = model.encode(sentence2)\n",
    "\n",
    "# Compute cosine similarity between the two sentences\n",
    "\n",
    "cosine_similarity([embedding1], [embedding2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp39-cp39-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.17-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.10-cp39-cp39-win_amd64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from datasets) (0.25.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp39-cp39-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.15.2-cp39-cp39-win_amd64.whl.metadata (58 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lucia\\documents\\github\\huggingface-demo\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp->datasets)\n",
      "  Downloading propcache-0.2.0-cp39-cp39-win_amd64.whl.metadata (7.9 kB)\n",
      "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading aiohttp-3.10.10-cp39-cp39-win_amd64.whl (381 kB)\n",
      "Downloading pyarrow-17.0.0-cp39-cp39-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.4/25.1 MB 12.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 4.5/25.1 MB 11.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.8/25.1 MB 11.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 8.9/25.1 MB 11.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 11.3/25.1 MB 11.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 13.9/25.1 MB 11.2 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.0/25.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/25.1 MB 11.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.7/25.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.5/25.1 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/25.1 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 10.8 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.4.1-cp39-cp39-win_amd64.whl (50 kB)\n",
      "Downloading multidict-6.1.0-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Downloading yarl-1.15.2-cp39-cp39-win_amd64.whl (84 kB)\n",
      "Downloading propcache-0.2.0-cp39-cp39-win_amd64.whl (45 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.9.0\n",
      "    Uninstalling fsspec-2024.9.0:\n",
      "      Successfully uninstalled fsspec-2024.9.0\n",
      "Successfully installed aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 async-timeout-4.0.3 attrs-24.2.0 datasets-3.0.1 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.6.1 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.0 pyarrow-17.0.0 xxhash-3.5.0 yarl-1.15.2\n"
     ]
    }
   ],
   "source": [
    "#huggingface also has some built in datasets\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes sure all input are the same length\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "\n",
    "uiuc=pd.read_csv(\"uiuc.csv\")\n",
    "mich=pd.read_csv(\"umich.csv\")\n",
    "\n",
    "#sample so we have even number of samples from each dataset\n",
    "mich=mich.sample(n=4725)\n",
    "\n",
    "#assign labels based on origin subreddit of comment\n",
    "uiuc['label']=1\n",
    "mich['label']=0\n",
    "\n",
    "#you will be working with the data csv for the rest of the question\n",
    "data=pd.concat([uiuc,mich])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset=Dataset.from_pandas(data[['text','label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7560/7560 [00:00<00:00, 8006.65 examples/s]\n",
      "Map: 100%|██████████| 1890/1890 [00:00<00:00, 8210.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'It’s a good tip for your entire lives.  There is no drawback. \\n\\nThat said, the profound discovery that daily bathing, much less wearing deodorant, is entirely avoided by a concerning portion of students who pile into the buses, will give a young person a sobering introduction to life’s weirdness if they hadn’t already experienced it before.',\n",
       " 'label': 1,\n",
       " '__index_level_0__': 431,\n",
       " 'input_ids': [101,\n",
       "  2009,\n",
       "  1521,\n",
       "  1055,\n",
       "  1037,\n",
       "  2204,\n",
       "  5955,\n",
       "  2005,\n",
       "  2115,\n",
       "  2972,\n",
       "  3268,\n",
       "  1012,\n",
       "  2045,\n",
       "  2003,\n",
       "  2053,\n",
       "  4009,\n",
       "  5963,\n",
       "  1012,\n",
       "  2008,\n",
       "  2056,\n",
       "  1010,\n",
       "  1996,\n",
       "  13769,\n",
       "  5456,\n",
       "  2008,\n",
       "  3679,\n",
       "  17573,\n",
       "  1010,\n",
       "  2172,\n",
       "  2625,\n",
       "  4147,\n",
       "  2139,\n",
       "  7716,\n",
       "  18842,\n",
       "  2102,\n",
       "  1010,\n",
       "  2003,\n",
       "  4498,\n",
       "  9511,\n",
       "  2011,\n",
       "  1037,\n",
       "  7175,\n",
       "  4664,\n",
       "  1997,\n",
       "  2493,\n",
       "  2040,\n",
       "  8632,\n",
       "  2046,\n",
       "  1996,\n",
       "  7793,\n",
       "  1010,\n",
       "  2097,\n",
       "  2507,\n",
       "  1037,\n",
       "  2402,\n",
       "  2711,\n",
       "  1037,\n",
       "  17358,\n",
       "  2075,\n",
       "  4955,\n",
       "  2000,\n",
       "  2166,\n",
       "  1521,\n",
       "  1055,\n",
       "  6881,\n",
       "  2791,\n",
       "  2065,\n",
       "  2027,\n",
       "  2910,\n",
       "  1521,\n",
       "  1056,\n",
       "  2525,\n",
       "  5281,\n",
       "  2009,\n",
       "  2077,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "#load accuracy evaluation\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "#default training parameters, don't worry about these for now\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all previous defined parameters into a trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [01:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[145], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#run training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucia\\Documents\\GitHub\\huggingface-demo\\.conda\\lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucia\\Documents\\GitHub\\huggingface-demo\\.conda\\lib\\site-packages\\transformers\\trainer.py:2452\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2448\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_pre_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2452\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2456\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n",
      "File \u001b[1;32mc:\\Users\\lucia\\Documents\\GitHub\\huggingface-demo\\.conda\\lib\\site-packages\\accelerate\\optimizer.py:171\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucia\\Documents\\GitHub\\huggingface-demo\\.conda\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lucia\\Documents\\GitHub\\huggingface-demo\\.conda\\lib\\site-packages\\torch\\optim\\optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m             )\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lucia\\Documents\\GitHub\\huggingface-demo\\.conda\\lib\\site-packages\\torch\\optim\\optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\lucia\\Documents\\GitHub\\huggingface-demo\\.conda\\lib\\site-packages\\torch\\optim\\adamw.py:227\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    217\u001b[0m         group,\n\u001b[0;32m    218\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m         state_steps,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[1;32m--> 227\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\lucia\\Documents\\GitHub\\huggingface-demo\\.conda\\lib\\site-packages\\torch\\optim\\optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lucia\\Documents\\GitHub\\huggingface-demo\\.conda\\lib\\site-packages\\torch\\optim\\adamw.py:767\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    765\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 767\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucia\\Documents\\GitHub\\huggingface-demo\\.conda\\lib\\site-packages\\torch\\optim\\adamw.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#run training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
